---
title: "Supervised Machine Learning: Regression and Classification"
author: Adrià
date: 2022-07-26 14:10:00 +0800
categories: [Machine Learning]
tags: [Machine_Learning]
render_with_liquid: false
---

# Supervised Machine Learning: Regression and Classification

A definition of Machine Learning is the field of study that gives computers tha ability to learn without being explicitly programmed. 

## Supervised vs. Unsupervised Machine Learning

### Supervised Learning

Algorithims that know x —> y  or input to output  mappings. This means that you give your learning algorithms examples to learn from, giving the inputs **(x)** and telling the desired output **(y)**. 

Regression and Classification algorithms use supervised learning. 

Regression models are used to predict continuous values, such as the price of a house or the weight of a person. These models are trained using labeled data, where the output or label is a continuous value. The goal of a regression model is to learn a mapping from inputs (also known as features) to the continuous output. Common examples of regression models include linear regression and polynomial regression.

Classification models are used to predict categorical values, such as whether an email is spam or not, or which type of fruit is in a picture. These models are also trained using labeled data, where the output or label is a categorical value. The goal of a classification model is to learn a mapping from inputs to the categorical output. Common examples of classification models include logistic regression, decision trees, and support vector machines.

### Unsupervised Learning

Unsupervised learning is a type of machine learning where the model is not given any labeled data. Instead, the goal is to find patterns or relationships in the data. Examples of unsupervised learning include clustering, dimensionality reduction, and anomaly detection. Clustering is the process of grouping similar data points together, dimensionality reduction is the process of reducing the number of features in the data while retaining important information, and anomaly detection is the process of identifying data points that do not conform to expected patterns.


## Linear Regression with one feature

In this part we will discuss more about the Spervised Model - Linear Regression. This model consist on fitting a straight line to our data that will predict the value. The straight line is calculeted based on the **training data** provided. 

![Linear-Regression-model-sample-illustration.png](/img/posts/SupervisedML/Linear_Regression_with_one_feature/Linear-Regression-model-sample-illustration.png)

When refering the data, this notation is used: $(x^{(i)},y^{(i)})$ where *i* is the *ith* iteration of the test data used, *x* is the input and *y* the output. 

With the training data set and a learning algorithm, we will create a function that when a new input value *x* is given , it calculates a result $\hat{y}$. This $\hat{y}$ wants to be as approximeted as *y* (target value) as possible

For the Linear Regression Model, the function is: 

$$ f_{w,b}(x) = wx + b $$

In Liniar Regression **w** is the **slope** of the line that the model will create. 

## Cost function formula

The cost function formula can tell us how the model is performing so we can upgrade it. 

In machine learning parameters of the model are the variables you can adjust during training in order to improve the model. In the Liniar Regression model, the parameters are *w* and *b.* Parameters are also refered as **coefficients** or **weights**. 

Once we have our model $ f_{w,b}(x) $, we can use it tu calculate $ \hat{y} $. A $ \hat{y}^{(i)} $ is obtained when using the model with the *ith* input:

$$ \hat{y}^{(i)} = f_{w,b}(x^{(i)}) = wx^{(i)} + b $$

But we want $ \hat{y}^{(i)} $ to be close to $ y^{(i)} $ for all $ (x^{(i)},y^{(i)}) $, in other words, we want our model to predict a value that is close to the real value for each test data. 

To mesure how well our model is performing, we have to calculate the cost function (**squared error cost function**). To do this we will calculate the square root of all the errors obtained with the training examples: 

$$ J(w,b)=\frac 1 {2m} \sum_{i=1}^m (\hat{y}^{(i)} - y^{(i)})^2 $$

This is the same as saying: 

$$ J(w,b)=\frac 1 {2m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})^2 $$

The goal is to minimize the value of this cost function $ J(w,b) $. If it is impossible to fit a single line that fits all the values, this cost will never reach 0. The 0 cost will only appear if it is possible to have a line that fits all the training data points. 

If we wanted to plot this cost function for this case, since it has two parametters (w  and b), the plot will have 3 Dimensions. In this example image, assume that $ \theta_0 = 2\space  and \space  \theta_1 = b $ 

 

![https://i.stack.imgur.com/cYAx7.png](https://i.stack.imgur.com/cYAx7.png)

Since we want to minimize the **cost** ($ J(w,b) $), we would be aiming for a value is in the bottom of this bowl. 

We could also plot this in 2d un function of w and b. In this plot, each line represents the same **cost.** You haave to imagine it like the plot of the bowl has been cuttet horizontaly and seen from above. 

[https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSxVpU-kad_GI1fzATSGnTmaLW9Qq_aIYnFUT9OTHeGZk_FsErRJzbpzOWCNM6w6wDhVCg&usqp=CAU](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSxVpU-kad_GI1fzATSGnTmaLW9Qq_aIYnFUT9OTHeGZk_FsErRJzbpzOWCNM6w6wDhVCg&usqp=CAU)

The red point in this plot implies a minimum (or low) cost, because the small circle represents the bottom of the bowl. 

**But, why when we are calculating the cost, we square the error?** One reason is that the squared error function is differentiable and convex, meaning that it has a unique **global minimum** (minimum **cost**) that can be found using optimization techniques such as **gradient descent**. Additionally, squaring the error term helps to penalize larger errors more heavily, which is often desired in many applications. 

# Gradient descent

Ok, but how we can calculate the values of **w** and **b** that minimize the **cost**? The gradient descent algorithm can help us.

It works by iteratively adjusting the parameters of a model in the direction of the negative gradient of the cost function with respect to those parameters. In other words, it moves the model in the direction that most reduces the cost.

Following the liniar regression example, we will have to adjust to parameters, w and b.  They will be updated using this criteria: 

Until they converge (the update is no longer noticable): 

$$ w = w - \alpha \space \frac \partial {\partial w} J(w,b) $$

$$ b = b - \alpha \space \frac \partial {\partial b} J(w,b) $$

The $ \alpha $ is the learning rate, but we will talk about it later. 

Bear in mind that if you are coding this, you dont wan’t $ **b** $  to get calculated using the new $ **w** $, you will ned to do something like this to avoid the problem: 

$$ temp\_w = w - \alpha \space \frac \partial {\partial w} J(w,b) $$

$$ temp\_b = b - \alpha \space \frac \partial {\partial b} J(w,b) $$

$ w = temp\_w $

$ b = temp\_b $

If we extend this functions calculating the derivatives, it we will have this: 

$$ w = w - \alpha \space \frac 1 {2m} \sum_{i=1}^m (wx^{(i)} + b - y^{(i)}) \space 2x^{(i)} =  w - \alpha \space \frac 1 {m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)}) \space x^{(i)} $$

$$ b = b - \alpha \space \frac 1 {2m} \sum_{i=1}^m (wx^{(i)} + b - y^{(i)}) \space 2 = w - \alpha \space \frac 1 {m} \sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)}) $$

Before talking about $\alpha$ , I would like to explain why the gradient descent works like this. 

The parameters **w** and **b** are being updated by their actual value but substracting the derivated value of the **cost** multiplied by the **learning rate.** 

To understand the impact of this better, assume that we will only considere one parameter of the cost formula, so the plot will be in 2D. When we calculate the partial derivative of a function respect a relative point **w**, we are calculating the slope of that point. If the w is on the right side of the bowl plot, the slope will be positive, so the result of $ \frac \partial {\partial w} J(w,b) $ will be positive. If we substract a positive number to the actual **w** or **b**, the new values will be smaller (each iteration will be more close to 0, which is our goal). 

However, if w is on the left side, the tangent line (slope) will have a negative value (second case in the image), the result $ \frac \partial {\partial w} J(w,b) $ will be negative. Substracting a negative number will increase the new values of w, that will produce a new **cost** closer to 0 too. 

Keeping it short, updating w and b by substracting their derivatives will give us new values that will imply a cost near 0.  

![derivative.jpg](/img/posts/SupervisedML/Linear_Regression_with_one_feature/derivative.jpg)

Now, lets talk about $ \alpha $ (learning rate). This value multiplies the value that we will use to update w and b. If we set a really low $ \alpha $, each iteration of the gradient descent will update the values very slowly and it will take more iterations ti reach the desired value. However, setting a large /alpha is worst. If we hevily update the values we may exceed the desired value and the cost will go from negative to positive values, increasing the absolute value (second case):

![learningrate.jpg](/img/posts/SupervisedML/Linear_Regression_with_one_feature/learningrate.jpg)

Now we know how to create a liniar regression model and use the gradient descent function to obtain the best values to use in our model.

